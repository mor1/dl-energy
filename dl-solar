#!/usr/bin/env python3

# Copyright 2024 Richard Mortier <mort@cantab.net>
# SPDX-License-Identifier: BSD-3-Clause
import asyncio
import base64
import csv
import datetime
import hashlib
import json
import logging
import pathlib
import sys
import time

import click
import httpx
import tomllib
from rich.logging import RichHandler

logging.basicConfig(format="%(message)s", datefmt="[%X]", handlers=[RichHandler()])
log = logging.getLogger(__name__)


def fail(s: str, n: int = -1) -> None:
    log.critical(s)
    sys.exit(n)


def check_date(d: str):
    try:
        datetime.date.fromisoformat(d)
    except ValueError:
        fail(f"invalid date: {d=}")

    return


def fmt(r: dict | None) -> str:
    return json.dumps(r, indent=2)


async def get(url: str, headers: dict, params: dict = {}, auth=None) -> dict:
    async with httpx.AsyncClient() as session:
        response = await session.get(url, headers=headers, params=params, auth=auth)
        log.debug(f"get: {response=}")
        data = response.json()
        match response.status_code:
            case 200:
                log.debug(f"get: {data=}")
                return data
            case _:
                log.debug(f"get: {response=}")
                fail(f"get: {response.status_code=} {data=}")
    assert False
    return {}  # unreached


def writecsv(filename, header, fieldnames, rows):
    pathlib.Path(filename).parent.mkdir(exist_ok=True, parents=True)

    with open(f"{filename}", "w") as tsvf:
        tsvf.write(header)
        tsvw = csv.DictWriter(
            tsvf,
            fieldnames=fieldnames,
            extrasaction="raise",
            delimiter="\t",
            dialect="excel",
        )
        tsvw.writeheader()
        for row in rows:
            tsvw.writerow(row)


class Source:
    def __init__(self, basedir: str, secrets: dict, date: str, now: str) -> None:
        assert False

    async def fetch(self) -> dict:
        assert False

    def dump(self, data: dict) -> None:
        assert False


class AlphaESS(Source):
    def __init__(self, basedir, secrets, date, now):
        self.timestamp = now
        self.secrets = secrets
        self.date = date
        self.filename = f"{basedir}/alphaess/{date}.tsv"

    def sign(self) -> str:
        appid = self.secrets["appid"]
        secret = self.secrets["secret"]
        timestamp = self.timestamp
        s = "%s%s%s" % (appid, secret, timestamp)
        signature = str(hashlib.sha512((s).encode("ascii")).hexdigest())
        log.debug(f"AlphaESS.sign: {appid=} {timestamp=} {signature=}")
        return signature

    def request(self, endpoint):
        log.debug(f"AlphaESS.request: {endpoint=}")
        url = f"https://openapi.alphaess.com/api/{endpoint}"
        headers = {
            "appId": self.secrets["appid"],
            "timeStamp": self.timestamp,
            "sign": self.sign(),
        }
        return url, headers

    async def fetch(self):
        data = {}

        url, headers = self.request("getEssList")
        params = {"queryDate": self.date}
        response = await get(url, headers, params)
        log.debug(f"fetch: {response=}")
        for sn in (i["sysSn"] for i in response["data"]):
            params["sysSn"] = sn

            url, headers = self.request("getOneDayPowerBySn")
            power = await get(url, headers, params)  # system, day's energy consumption

            url, headers = self.request("getOneDateEnergyBySn")
            energy = await get(url, headers, params)

            # XXX must run as a service; unclear update frequency
            # recent = await _get("getLastPowerData")
            # log.info(f"{recent=}")

            data[sn] = {
                "power": power["data"],
                "energy": energy["data"],
            }

        log.debug(f"fetch: {fmt(data)}")
        return data

    def dump(self, data: dict):
        assert len(data.keys()) == 1
        data = list(data.values())[0]

        fieldsmap = {
            "ts": "ts",
            "uploadTime": "time",
            "ppv": "from-pv(Wh)",
            "gridCharge": "from-grid(Wh)",
            "load": "house-load(Wh)",
            "feedIn": "to-grid(Wh)",
            "to-battery(Wh)": "to-battery(Wh)",
            "from-battery(Wh)": "from-battery(Wh)",
            # "pchargingPile": "to-battery(Wh)", # XXX constant 0
            "cbat": "battery-charge(%)",
        }
        power = [
            {fieldsmap[k]: v for k, v in row.items() if fieldsmap.get(k)}
            for row in data["power"]
        ]
        for row in power:
            dt = datetime.datetime.fromisoformat(row["time"]).astimezone()
            row["ts"] = int(dt.timestamp())
            row["time"] = dt.isoformat()
            generated = row["from-pv(Wh)"] + row["from-grid(Wh)"]
            load = row["to-grid(Wh)"] + row["house-load(Wh)"]
            diff = generated - load
            row["to-battery(Wh)"] = diff if diff > 0 else 0
            row["from-battery(Wh)"] = -1 * diff if diff < 0 else 0
        power = sorted(power, key=lambda row: row["ts"])

        energy = data["energy"]
        header = (
            f"# {energy['theDate']}"
            f"\tto-battery(kWh)={energy['eCharge']}"
            f"\tfrom-battery(kWh)={energy['eDischarge']}"
            f"\tto-grid(kWh)={energy['eDischarge']}"
            f"\tfrom-grid(kWh)={energy['eInput']}"
            f"\tfrom-pv(kWh)={energy['epv']}"
            "\n"
        )
        writecsv(self.filename, header, fieldsmap.values(), power)


class Octopus(Source):
    # https://docs.octopus.energy/rest/reference
    def __init__(self, basedir, secrets, date, now):
        self.timestamp = now
        self.secrets = secrets
        self.date = date
        self.filename = f"{basedir}/octopus/{date}.tsv"

    async def fetch(self) -> dict:
        mpan = self.secrets["mpan"]
        meter = self.secrets["meter"]
        auth = base64.b64encode(f"{self.secrets['apikey']}".encode()).decode()

        url = f"https://api.octopus.energy/v1/electricity-meter-points/{mpan}/meters/{meter}/consumption/"
        headers = {
            "Authorization": f"Basic {auth}",  # basic auth, by hand
        }
        params = {
            "page_size": 100,  # >> number of half-hour periods in 24h
            "period_from": f"{self.date}T00:00:00Z",
            "period_to": f"{self.date}T23:59:59Z",
            "order_by": "period",
        }
        response = await get(url, headers, params)
        log.debug(f"{fmt(response)}")
        assert response["count"] <= 48
        return response["results"]

    def dump(self, data) -> None:
        log.debug(f"dump: {fmt(data)}")

        header = ""
        fieldsmap = {
            "interval_start": "ts",
            "consumption": "from-grid(Wh)",
        }
        rows = sorted(
            (
                {
                    "ts": v["interval_start"],
                    "from-grid(Wh)": v["consumption"],
                }
                for v in data
            ),
            key=lambda row: row["ts"],
        )

        writecsv(self.filename, header, fieldsmap.values(), rows)


class MyEnergi(Source):
    # https://github.com/twonk/MyEnergi-App-Api
    def __init__(self, basedir, secrets, date, now):
        self.timestamp = now
        self.secrets = secrets
        self.date = date
        self.filename = f"{basedir}/myenergi/{date}.tsv"

    async def fetch(self) -> dict:
        serial = self.secrets["serial"]
        apikey = self.secrets["apikey"]

        url = f"https://s18.myenergi.net/cgi-jday-Z{serial}-{self.date}"
        headers = {
            "Accept": "application/json",
            "Content-type": "application/json",
        }
        auth = httpx.DigestAuth(serial, apikey)
        response = await get(url, headers, auth=auth)
        log.debug(f"MyEnergi.fetch: {response=}")

        return response[f"U{serial}"]

    def dump(self, data) -> None:
        log.debug(f"{fmt(data)}")
        assert len(data) <= 1441

        for v in data:
            for k in ("hr", "min", "imp", "exp"):
                if k not in v:
                    v[k] = 0
            isotime = (
                f"{v['yr']}-{v['mon']:02}-{v['dom']:02}T{v['hr']:02}:{v['min']:02}"
            )
            dt = datetime.datetime.fromisoformat(isotime).astimezone()
            v["ts"] = int(dt.timestamp())
            v["time"] = dt.isoformat()

        fieldsmap = {
            "ts": "ts",
            "time": "time",
            "exp": "to-grid(J)",
            "imp": "from-grid(J)",
            "v1": "voltage(V)",
            "frq": "frequency(Hz)",
        }
        rows = sorted(
            [
                {fieldsmap[k]: v for k, v in row.items() if fieldsmap.get(k)}
                for row in data
            ],
            key=lambda row: row["ts"],
        )

        writecsv(self.filename, "", fieldsmap.values(), rows)


async def main(sources: list) -> None:
    for source in sources:
        data = await source.fetch()
        source.dump(data)


Sources = {
    "alphaess": AlphaESS,
    "octopus": Octopus,
    "myenergi": MyEnergi,
}


@click.command()
@click.option("-l", "--loglevel", default="INFO", help="Set log level.")
@click.option(
    "-s",
    "--source",
    "ss",
    multiple=True,
    default=Sources.keys(),
    help="Data source.",
)
@click.argument("date")
def cli(
    loglevel: str,
    ss: str,
    date: str,
):
    logging.getLogger().setLevel(loglevel.upper())
    with open("dl-solar.toml", "rb") as f:
        config = tomllib.load(f)
    log.debug(f"{config=}")

    if date == "yesterday":
        date = (datetime.date.today() - datetime.timedelta(1)).isoformat()
    check_date(date)
    log.info(f"{date=}")

    now = str(int(time.time()))

    sources: list[Source] = [
        Sources[s](config["basedir"], config["secrets"][s], date, now) for s in ss
    ]

    asyncio.run(main(sources))


if __name__ == "__main__":
    cli()
